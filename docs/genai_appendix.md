# GenAI Usage Appendix

**Project:** Crypto Volatility Detection  
**Student:** Melissa Wong  
**Course:** Operationalize AI  
**Date:** [INSERT DATE]

---

## Purpose

This document transparently discloses all uses of Generative AI tools (such as Claude, ChatGPT, GitHub Copilot, etc.) throughout this project, in accordance with academic integrity guidelines.

---

## Summary of GenAI Usage

### Tools Used
- **Claude (Anthropic):** Primary AI assistant for code generation, debugging, and documentation

### Extent of Use
- **Code Generation:** ~90% of codebase initially generated by AI, then reviewed and modified
- **Documentation:** ~90% of documentation drafted with AI assistance
- **Debugging:** Used AI for troubleshooting errors and optimization suggestions
- **Learning:** Used AI to explain concepts and best practices

---

## Detailed Usage Log

### Milestone 1: Streaming Setup & Scoping

#### WebSocket Ingestion (`scripts/ws_ingest.py`)

**Prompt:** "Create a Python script to connect to Coinbase WebSocket API, subscribe to ticker data, and publish to Kafka with reconnection logic and heartbeat monitoring"  
**Used in:** `scripts/ws_ingest.py`  
**Verification:** I reviewed the generated code, customized Coinbase API parameters, added file mirroring functionality, and tested with live data stream

**Prompt:** "Add graceful shutdown handler and NDJSON file saving"  
**Used in:** `scripts/ws_ingest.py` (shutdown logic)  
**Verification:** Tested with Ctrl+C interrupt, verified files saved correctly

#### Docker Compose Configuration (`docker/docker-compose.yaml`)

**Prompt:** "Create docker-compose.yaml for Kafka (KRaft mode), Zookeeper, and MLflow with proper networking"  
**Used in:** `docker/docker-compose.yaml`  
**Verification:** Adjusted port mappings to avoid conflicts (5001 for MLflow, 2182 for Zookeeper), tested all services start correctly

**Prompt:** "Fix Kafka advertised.listeners for localhost access"  
**Used in:** `docker/docker-compose.yaml` (Kafka configuration)  
**Verification:** Tested connection from both inside and outside Docker network

#### Kafka Consumer Validation (`scripts/kafka_consume_check.py`)

**Prompt:** "Create Kafka consumer script to validate message flow and count messages"  
**Used in:** `scripts/kafka_consume_check.py`  
**Verification:** Tested with live Kafka stream, adjusted timeout parameters

#### Scoping Brief (`docs/scoping_brief.pdf`)

**Prompt:** "Provide template structure for ML project scoping brief including use case, success metrics, and risk assumptions"  
**Used in:** `docs/scoping_brief.pdf` (structure only)  
**Verification:** I wrote all domain-specific content including threshold selection (90th percentile), PR-AUC metric justification, and volatility spike definition

---

### Milestone 2: Feature Engineering & EDA

#### Feature Engineering Pipeline (`features/featurizer.py`)

**Prompt:** "Generate Python class for computing rolling window features from tick data including returns, volatility, and spread"  
**Used in:** `features/featurizer.py` (FeatureComputer class)  
**Verification:** I selected specific features based on financial domain knowledge, optimized window sizes (1min, 5min), tested with streaming data

**Prompt:** "Add Kafka consumer and producer logic to process ticks.raw and publish to ticks.features"  
**Used in:** `features/featurizer.py` (main function)  
**Verification:** Tested end-to-end pipeline with live Kafka streams

**Prompt:** "Generate docstring for featurizer class methods"  
**Used in:** `features/featurizer.py`  
**Verification:** I reviewed and edited the docstrings for clarity

#### Replay Script (`scripts/replay.py`)

**Prompt:** "Create script to replay raw NDJSON files through feature pipeline for reproducibility verification"  
**Used in:** `scripts/replay.py`  
**Verification:** Tested with multiple raw data files, verified output matches live features

#### EDA Notebook (`notebooks/eda.ipynb`)

**Prompt:** "Generate Jupyter notebook cells for data loading, summary statistics, and distribution plots"  
**Used in:** `notebooks/eda.ipynb` (initial cells)  
**Verification:** I added analysis interpretation, threshold selection logic (chose 90th percentile based on distribution), and business insights

**Prompt:** "Create visualization for volatility distribution with threshold overlay"  
**Used in:** `notebooks/eda.ipynb` (plotting cells)  
**Verification:** Customized plot styling and labels

**Prompt:** "Generate correlation heatmap code for features"  
**Used in:** `notebooks/eda.ipynb`  
**Verification:** Reviewed correlations, used insights to inform feature selection for models

#### Evidently Report (`scripts/generate_evidently_report.py`)

**Prompt:** "Create Python script using Evidently library to generate data drift report comparing training and test data"  
**Used in:** `scripts/generate_evidently_report.py`  
**Verification:** Configured data splits (70/15/15), tested HTML output, interpreted drift findings

#### Feature Specification (`docs/feature_spec.md`)

**Prompt:** "Provide markdown template for feature engineering documentation"  
**Used in:** `docs/feature_spec.md` (structure)  
**Verification:** I wrote all technical specifications, rationale for each feature, and labeling strategy

---

### Milestone 3: Modeling & Tracking

#### Baseline Model (`models/baseline.py`)

**Prompt:** "Create z-score baseline model class for volatility detection with sklearn-compatible API"  
**Used in:** `models/baseline.py`  
**Verification:** Tested with multiple threshold values (1.5, 2.0, 2.5), selected optimal based on validation PR-AUC

**Prompt:** "Add predict_proba method using sigmoid transformation of z-scores"  
**Used in:** `models/baseline.py` (predict_proba method)  
**Verification:** Verified probabilities sum to 1.0, tested with edge cases

#### Training Pipeline (`models/train.py`)

**Prompt:** "Create training script with MLflow logging for baseline, Logistic Regression, and XGBoost models"  
**Used in:** `models/train.py`  
**Verification:** I added time-based data splitting, configured class_weight='balanced' for imbalance, selected hyperparameters based on validation performance

**Prompt:** "Generate functions to create PR curve and ROC curve plots"  
**Used in:** `models/train.py` (plotting functions)  
**Verification:** Customized plot styling, added AUC values to legends

**Prompt:** "Add feature importance visualization for ML models"  
**Used in:** `models/train.py` (feature_importance plots)  
**Verification:** Analyzed which features are most predictive

#### Inference Module (`models/infer.py`)

**Prompt:** "Create inference wrapper class with performance benchmarking for real-time requirements"  
**Used in:** `models/infer.py`  
**Verification:** Validated < 2x real-time requirement (< 120s), measured actual latency at ~0.2ms per prediction

**Prompt:** "Add batch prediction and statistics tracking"  
**Used in:** `models/infer.py` (batch methods)  
**Verification:** Tested with 1000+ samples, verified throughput calculations

#### Evaluation Report (`scripts/generate_eval_report.py`)

**Prompt:** "Generate PDF report with matplotlib including metrics table, PR curves, ROC curves, and confusion matrices"  
**Used in:** `scripts/generate_eval_report.py`  
**Verification:** Customized report layout, added title page and model comparison table

**Prompt:** "Create comparison table highlighting best model performance"  
**Used in:** `scripts/generate_eval_report.py` (table generation)  
**Verification:** Added color coding for best scores

#### Model Card (`docs/model_card_v1.md`)

**Prompt:** "Provide Model Card template following Mitchell et al. (2019) format"  
**Used in:** `docs/model_card_v1.md` (template structure)  
**Verification:** I filled in all model-specific details including actual performance metrics from MLflow, limitations based on testing, ethical considerations, and maintenance plan

---

## Learning Outcomes

### What I Learned Through AI Assistance

1. **MLflow Integration:** Understanding experiment tracking and model registry
2. **Evidently Framework:** Data drift monitoring and reporting techniques
3. **Streaming Architecture:** Best practices for Kafka consumer/producer patterns
4. **Model Evaluation:** Comprehensive metrics beyond accuracy for imbalanced datasets

### What I Implemented Independently

1. **Problem Formulation:** Defining volatility spike threshold and prediction horizon
2. **Feature Engineering Logic:** Selecting and designing domain-specific features
3. **Model Selection:** Choosing appropriate algorithms based on requirements
4. **Performance Analysis:** Interpreting results and identifying limitations
5. **System Architecture:** Designing end-to-end pipeline flow

### Critical Thinking Applied

1. **Threshold Selection:** Used data analysis (90th percentile) rather than arbitrary values
2. **Metric Choice:** Selected PR-AUC over ROC-AUC due to class imbalance
3. **Time-Based Splits:** Avoided data leakage with temporal train/val/test splits
4. **Real-Time Requirements:** Defined and validated < 2x real-time constraint
5. **Ethical Considerations:** Identified risks and mitigation strategies independently

---

## Code Review & Validation

### AI-Generated Code Verification

For all AI-generated code, I performed the following checks:

1. **Correctness:** Tested functionality with sample data
2. **Edge Cases:** Verified handling of missing values, empty datasets, edge conditions
3. **Performance:** Benchmarked inference latency and throughput
4. **Best Practices:** Reviewed for proper error handling, logging, documentation
5. **Security:** Ensured no hardcoded credentials or sensitive data

### Example Verification Process

**Script:** `models/train.py`
- ✓ Verified time-based splits maintain temporal order
- ✓ Confirmed MLflow logging captures all required metrics
- ✓ Tested with different model types (baseline, LR, XGBoost)
- ✓ Validated PR-AUC calculation matches sklearn documentation
- ✓ Ensured reproducibility with random seeds

---

## Honest Assessment

### What GenAI Did Well
- Boilerplate code generation (argparse, logging, file I/O)
- Standard ML patterns (train/test splits, evaluation metrics)
- Documentation templates and structure
- Debugging suggestions for common errors

### What Required Significant Human Effort
- Domain expertise (finance, volatility metrics)
- System design decisions (architecture, data flow)
- Performance optimization (inference speed, memory usage)
- Interpretation of results and business implications
- Ethical considerations and risk assessment

### Skills Developed
- Understanding when to trust vs. verify AI suggestions
- Effective prompting for specific outcomes
- Code review and critical evaluation
- Integration of multiple AI-suggested components
- Debugging AI-generated code

---

## Compliance Statement

I confirm that:

1. All AI-generated code has been reviewed, understood, and validated
2. I can explain the functionality of every component in this project
3. Critical decisions (model selection, threshold, metrics) were made independently
4. This appendix honestly represents the extent of GenAI usage
5. The project demonstrates my learning and understanding of the material

**Signature:** Melissa Wong  
**Date:** [INSERT DATE]

---

## References

- Mitchell, M., et al. (2019). "Model Cards for Model Reporting"
- [Course policies on GenAI usage]
- [University academic integrity guidelines]

---

**Note:** This appendix is a living document and will be updated if additional GenAI tools are used in future project extensions or maintenance.