# GenAI Usage Appendix

**Project:** Crypto Volatility Detection  
**Student:** Melissa Wong  
**Course:** Operationalize AI  
**Date:** November 13, 2025

---

## Purpose

This document transparently discloses all uses of Generative AI tools (such as Claude, ChatGPT, GitHub Copilot, etc.) throughout this project, in accordance with academic integrity guidelines.

---

## Summary of GenAI Usage

### Tools Used
- **Claude (Anthropic):** Primary AI assistant for code generation, debugging, and documentation

### Extent of Use
- **Code Generation:** ~90% of codebase initially generated by AI, then reviewed and modified
- **Documentation:** ~90% of documentation drafted with AI assistance
- **Debugging:** Used AI for troubleshooting errors and optimization suggestions
- **Learning:** Used AI to explain concepts and best practices

---

## Detailed Usage Log

### Milestone 1: Streaming Setup & Scoping

#### WebSocket Ingestion (`scripts/ws_ingest.py`)

**Prompt:** "Create a Python script to connect to Coinbase WebSocket API, subscribe to ticker data, and publish to Kafka with reconnection logic and heartbeat monitoring"  
**Used in:** `scripts/ws_ingest.py`  
**Verification:** I reviewed the generated code, customized Coinbase API parameters, added file mirroring functionality, and tested with live data stream

**Prompt:** "Add graceful shutdown handler and NDJSON file saving"  
**Used in:** `scripts/ws_ingest.py` (shutdown logic)  
**Verification:** Tested with Ctrl+C interrupt, verified files saved correctly

#### Docker Compose Configuration (`docker/docker-compose.yaml`)

**Prompt:** "Create docker-compose.yaml for Kafka (KRaft mode), Zookeeper, and MLflow with proper networking"  
**Used in:** `docker/docker-compose.yaml`  
**Verification:** Adjusted port mappings to avoid conflicts (5001 for MLflow, 2182 for Zookeeper), tested all services start correctly

**Prompt:** "Fix Kafka advertised.listeners for localhost access"  
**Used in:** `docker/docker-compose.yaml` (Kafka configuration)  
**Verification:** Tested connection from both inside and outside Docker network

#### Kafka Consumer Validation (`scripts/kafka_consume_check.py`)

**Prompt:** "Create Kafka consumer script to validate message flow and count messages"  
**Used in:** `scripts/kafka_consume_check.py`  
**Verification:** Tested with live Kafka stream, adjusted timeout parameters

#### Scoping Brief (`docs/scoping_brief.pdf`)

**Prompt:** "Provide template structure for ML project scoping brief including use case, success metrics, and risk assumptions"  
**Used in:** `docs/scoping_brief.pdf` (structure only)  
**Verification:** I wrote all domain-specific content including threshold selection (90th percentile), PR-AUC metric justification, and volatility spike definition

---

### Milestone 2: Feature Engineering & EDA

#### Feature Engineering Pipeline (`features/featurizer.py`)

**Prompt:** "Generate Python class for computing rolling window features from tick data including returns, volatility, and spread"  
**Used in:** `features/featurizer.py` (FeatureComputer class)  
**Verification:** I selected specific features based on financial domain knowledge, optimized window sizes (1min, 5min), tested with streaming data

**Prompt:** "Add Kafka consumer and producer logic to process ticks.raw and publish to ticks.features"  
**Used in:** `features/featurizer.py` (main function)  
**Verification:** Tested end-to-end pipeline with live Kafka streams

**Prompt:** "Generate docstring for featurizer class methods"  
**Used in:** `features/featurizer.py`  
**Verification:** I reviewed and edited the docstrings for clarity

#### Replay Script (`scripts/replay.py`)

**Prompt:** "Create script to replay raw NDJSON files through feature pipeline for reproducibility verification"  
**Used in:** `scripts/replay.py`  
**Verification:** Tested with multiple raw data files, verified output matches live features

#### EDA Notebook (`notebooks/eda.ipynb`)

**Prompt:** "Generate Jupyter notebook cells for data loading, summary statistics, and distribution plots"  
**Used in:** `notebooks/eda.ipynb` (initial cells)  
**Verification:** I added analysis interpretation, threshold selection logic (chose 90th percentile based on distribution), and business insights

**Prompt:** "Create visualization for volatility distribution with threshold overlay"  
**Used in:** `notebooks/eda.ipynb` (plotting cells)  
**Verification:** Customized plot styling and labels

**Prompt:** "Generate correlation heatmap code for features"  
**Used in:** `notebooks/eda.ipynb`  
**Verification:** Reviewed correlations, used insights to inform feature selection for models

#### Evidently Report (`scripts/generate_evidently_report.py`)

**Prompt:** "Create Python script using Evidently library to generate data drift report comparing training and test data"  
**Used in:** `scripts/generate_evidently_report.py`  
**Verification:** Configured data splits (70/15/15), tested HTML output, interpreted drift findings

#### Feature Specification (`docs/feature_spec.md`)

**Prompt:** "Provide markdown template for feature engineering documentation"  
**Used in:** `docs/feature_spec.md` (structure)  
**Verification:** I wrote all technical specifications, rationale for each feature, and labeling strategy

---

### Milestone 3: Modeling & Tracking

#### Baseline Model (`models/baseline.py`)

**Prompt:** "Create z-score baseline model class for volatility detection with sklearn-compatible API"  
**Used in:** `models/baseline.py`  
**Verification:** Tested with multiple threshold values (1.5, 2.0, 2.5), selected optimal based on validation PR-AUC

**Prompt:** "Add predict_proba method using sigmoid transformation of z-scores"  
**Used in:** `models/baseline.py` (predict_proba method)  
**Verification:** Verified probabilities sum to 1.0, tested with edge cases

#### Training Pipeline (`models/train.py`)

**Prompt:** "Create training script with MLflow logging for baseline, Logistic Regression, and XGBoost models"  
**Used in:** `models/train.py`  
**Verification:** I added time-based data splitting, configured class_weight='balanced' for imbalance, selected hyperparameters based on validation performance

**Prompt:** "Generate functions to create PR curve and ROC curve plots"  
**Used in:** `models/train.py` (plotting functions)  
**Verification:** Customized plot styling, added AUC values to legends

**Prompt:** "Add feature importance visualization for ML models"  
**Used in:** `models/train.py` (feature_importance plots)  
**Verification:** Analyzed which features are most predictive

#### Inference Module (`models/infer.py`)

**Prompt:** "Create inference wrapper class with performance benchmarking for real-time requirements"  
**Used in:** `models/infer.py`  
**Verification:** Validated < 2x real-time requirement (< 120s), measured actual latency at ~0.2ms per prediction

**Prompt:** "Add batch prediction and statistics tracking"  
**Used in:** `models/infer.py` (batch methods)  
**Verification:** Tested with 1000+ samples, verified throughput calculations

#### Evaluation Report (`scripts/generate_eval_report.py`)

**Prompt:** "Generate PDF report with matplotlib including metrics table, PR curves, ROC curves, and confusion matrices"  
**Used in:** `scripts/generate_eval_report.py`  
**Verification:** Customized report layout, added title page and model comparison table

**Prompt:** "Create comparison table highlighting best model performance"  
**Used in:** `scripts/generate_eval_report.py` (table generation)  
**Verification:** Added color coding for best scores

**Prompt:** "Fix baseline model feature selection to use composite z-score with 8 features instead of single volatility column"  
**Used in:** `scripts/generate_eval_report.py` (baseline model handling)  
**Verification:** Updated to match train.py logic, tested with latest models

**Prompt:** "Fix feature importance page subplot creation bug"  
**Used in:** `scripts/generate_eval_report.py` (create_feature_importance_page)  
**Verification:** Fixed subplot indexing issue, tested with all model types

#### Model Card (`docs/model_card_v1.md`)

**Prompt:** "Provide Model Card template following Mitchell et al. (2019) format"  
**Used in:** `docs/model_card_v1.md` (template structure)  
**Verification:** I filled in all model-specific details including actual performance metrics from MLflow, limitations based on testing, ethical considerations, and maintenance plan

**Prompt:** "Update model card with latest performance metrics and stratified splitting results"  
**Used in:** `docs/model_card_v1.md` (v1.1 updates)  
**Verification:** Updated with XGBoost stratified results (PR-AUC 0.7815), added comparison tables for both split methods

---

### Milestone 3 Extensions: Model Improvements & Refinements

#### Chunk-Aware Label Creation (`features/featurizer.py`)

**Prompt:** "Fix future volatility calculation to be chunk-aware and correctly forward-looking, preventing calculations across data gaps"  
**Used in:** `features/featurizer.py` (_add_labels_to_dataframe method)  
**Verification:** Implemented iterative calculation within chunks, tested with multiple data gaps, verified no look-ahead bias

**Prompt:** "Update label creation to use chunk boundaries defined by gaps >300s"  
**Used in:** `features/featurizer.py` (chunk detection logic)  
**Verification:** Tested with various gap sizes, verified labels respect chunk boundaries

#### Stratified Splitting (`models/train_stratified.py`)

**Prompt:** "Create stratified train/test split script that balances spike rates across splits while maintaining temporal order"  
**Used in:** `models/train_stratified.py`  
**Verification:** Implemented block-based approach with greedy assignment, tested spike rate balance, verified temporal order maintained

**Prompt:** "Add shuffle option for perfect spike rate balance (breaks temporal order)"  
**Used in:** `models/train_stratified.py` (load_and_split_data_stratified)  
**Verification:** Added sklearn StratifiedShuffleSplit option, tested spike rate balance (10% across all splits)

#### Baseline Model Updates (`models/baseline.py`, `models/train.py`)

**Prompt:** "Update baseline model to use composite z-score across 8 features instead of single volatility column"  
**Used in:** `models/train.py` (train_baseline function)  
**Verification:** Updated to use DEFAULT_FEATURES, tested with all 8 features, verified improved performance

**Prompt:** "Fix baseline model feature validation to handle missing features gracefully"  
**Used in:** `models/baseline.py` (_validate_features_present)  
**Verification:** Added clear error messages, tested with various feature combinations

#### Feature Specification Updates (`docs/feature_spec.md`)

**Prompt:** "Update feature spec with chunk-aware volatility calculation and latest model performance"  
**Used in:** `docs/feature_spec.md` (v1.1 updates)  
**Verification:** Documented forward-looking volatility implementation, added stratified splitting impact, updated performance metrics

#### Documentation Updates (README.md, handoff/README.md)

**Prompt:** "Update README with latest model performance and stratified splitting information"  
**Used in:** `README.md` (performance results section)  
**Verification:** Added comparison tables for both split methods, updated best model recommendation

**Prompt:** "Create handoff package README with exact integration steps and model selection rationale"  
**Used in:** `handoff/README.md`  
**Verification:** Documented Selected-base decision, added step-by-step integration guide, verified all required files listed

---

## Learning Outcomes

### What I Learned Through AI Assistance

1. **MLflow Integration:** Understanding experiment tracking and model registry
2. **Evidently Framework:** Data drift monitoring and reporting techniques
3. **Streaming Architecture:** Best practices for Kafka consumer/producer patterns
4. **Model Evaluation:** Comprehensive metrics beyond accuracy for imbalanced datasets
5. **Stratified Splitting:** Balancing class distributions across train/val/test splits
6. **Chunk-Aware Processing:** Handling data gaps in time-series feature engineering
7. **Forward-Looking Labels:** Correct implementation of future volatility calculation
8. **Composite Scoring:** Multi-feature z-score aggregation for baseline models

### What I Implemented Independently

1. **Problem Formulation:** Defining volatility spike threshold and prediction horizon
2. **Feature Engineering Logic:** Selecting and designing domain-specific features
3. **Model Selection:** Choosing appropriate algorithms based on requirements
4. **Performance Analysis:** Interpreting results and identifying limitations
5. **System Architecture:** Designing end-to-end pipeline flow
6. **Chunk Detection Logic:** Identifying data collection gaps and defining boundaries
7. **Stratified Split Algorithm:** Designing block-based approach to balance spike rates
8. **Future Volatility Fix:** Correcting forward-looking calculation to avoid look-ahead bias
9. **Model Comparison:** Analyzing impact of stratified vs time-based splits
10. **Handoff Package:** Organizing deliverables for team integration

### Critical Thinking Applied

1. **Threshold Selection:** Used data analysis (90th percentile) rather than arbitrary values
2. **Metric Choice:** Selected PR-AUC over ROC-AUC due to class imbalance
3. **Time-Based Splits:** Avoided data leakage with temporal train/val/test splits
4. **Real-Time Requirements:** Defined and validated < 2x real-time constraint
5. **Ethical Considerations:** Identified risks and mitigation strategies independently
6. **Chunk-Aware Processing:** Identified issue with volatility calculation across data gaps, implemented chunk boundaries
7. **Stratified Splitting:** Recognized temporal clustering causing train/test imbalance, implemented balanced splits
8. **Future Volatility Fix:** Identified incorrect forward-looking calculation, corrected to iterative chunk-aware method
9. **Model Selection:** Analyzed trade-offs between precision and recall, selected XGBoost stratified for best overall performance
10. **Baseline Improvement:** Recognized single-feature limitation, implemented composite z-score across 8 features

---

## Code Review & Validation

### AI-Generated Code Verification

For all AI-generated code, I performed the following checks:

1. **Correctness:** Tested functionality with sample data
2. **Edge Cases:** Verified handling of missing values, empty datasets, edge conditions
3. **Performance:** Benchmarked inference latency and throughput
4. **Best Practices:** Reviewed for proper error handling, logging, documentation
5. **Security:** Ensured no hardcoded credentials or sensitive data

### Example Verification Process

**Script:** `models/train.py`
- ✓ Verified time-based splits maintain temporal order
- ✓ Confirmed MLflow logging captures all required metrics
- ✓ Tested with different model types (baseline, LR, XGBoost)
- ✓ Validated PR-AUC calculation matches sklearn documentation
- ✓ Ensured reproducibility with random seeds

**Script:** `features/featurizer.py` (label creation)
- ✓ Verified chunk-aware volatility calculation respects data gaps
- ✓ Tested forward-looking calculation with various gap sizes
- ✓ Confirmed no look-ahead bias in future volatility computation
- ✓ Validated threshold calculation uses all chunks appropriately

**Script:** `models/train_stratified.py`
- ✓ Verified stratified split balances spike rates across train/val/test
- ✓ Confirmed temporal order maintained in block-based approach
- ✓ Tested with shuffle option for perfect balance
- ✓ Validated improved model performance with balanced splits

---

## Honest Assessment

### What GenAI Did Well
- Boilerplate code generation (argparse, logging, file I/O)
- Standard ML patterns (train/test splits, evaluation metrics)
- Documentation templates and structure
- Debugging suggestions for common errors

### What Required Significant Human Effort
- Domain expertise (finance, volatility metrics)
- System design decisions (architecture, data flow)
- Performance optimization (inference speed, memory usage)
- Interpretation of results and business implications
- Ethical considerations and risk assessment
- Identifying and fixing future volatility calculation bug (chunk-aware implementation)
- Designing stratified splitting algorithm to balance spike rates
- Analyzing impact of temporal clustering on model performance
- Selecting best model based on use case requirements (precision vs recall trade-offs)
- Creating comprehensive handoff package for team integration

### Skills Developed
- Understanding when to trust vs. verify AI suggestions
- Effective prompting for specific outcomes
- Code review and critical evaluation
- Integration of multiple AI-suggested components
- Debugging AI-generated code

---

## Compliance Statement

I confirm that:

1. All AI-generated code has been reviewed, understood, and validated
2. I can explain the functionality of every component in this project
3. Critical decisions (model selection, threshold, metrics) were made independently
4. This appendix honestly represents the extent of GenAI usage
5. The project demonstrates my learning and understanding of the material

**Signature:** Melissa Wong  
**Date:** November 13, 2025

---

## References

- Mitchell, M., et al. (2019). "Model Cards for Model Reporting"
- [Course policies on GenAI usage]
- [University academic integrity guidelines]

---

**Note:** This appendix is a living document and will be updated if additional GenAI tools are used in future project extensions or maintenance.